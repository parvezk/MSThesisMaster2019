
\chapter{BACKGROUND}

\graphicspath{ {./background/} }
%%%%%%%% This line gets rid of page number on first page of text
\thispagestyle{empty}

%%%%%%%%%%%%%

\section{The Rise of Machine Learning}

Machine learning now affects many aspects of our lives from web searches,  recommendations on e-commerce websites, smart speakers, movie recommendations, stock predictions to criminal justice and healthcare systems. It is omnipotent in consumer products such as cameras, smartphones and personal assistants. There is a huge incentive for businesses to apply machine learning from enhanced decision making, business forecast, cost reduction, risk management, productivity improvements, as well as the development of new products and services.

Machine learning is a branch of computer science that uses statistical techniques to give computer systems the ability to learn from data, find patterns in data and make highly accurate predictions. In other words, its a set of techniques used to teach computers how to learn, reason, perceive, distinguish, infer, communicate and make decisions like human do. It is a large part of modern artificial intelligence. Here, data encompasses many things such as words, numbers, images, video and more. Machine learning learns from the experience, following a pathway that nature took millions of year ago \cite{sammut2011encyclopedia}. It's a paradigm shift from the 'normal' programming where all the instructions must be given explicitly to the computer to 'indirect' programming where the computer learns itself from the data without being explicitly programmed .

Not very long ago, to say that computer vision could compete with the visual abilities of a one-year-old was seemingly a paradox\cite{sejnowski2018deep}. It is no longer valid. Machine learning techniques can now recognize objects in an image as well as an adult can, digitize handwritten characters, transcribe speech into text, classify wine types, support diagnosis of severe diseases, match news item or product with user's interest and select results relevant to the search. Then there are self-driving cars \cite{sejnowski2018deep} on the road that can drive safely than an average person. 

What is fuelling these advances is the recent surge in data and the rise in computing power over the past few years, which has led to breakthroughs in the field of machine learning and deep learning.

\subsection{Data Upsurge}
The exponential growth in data in recent times has fueled machine learning based industries, technologies, and services. Nonetheless, humans and machines are generating more data today than ever before. Every day, humans alone produce a massive amount of data ranging from text, audio, video, sensory data and more, and that number is expected to proliferate in the decades to come. 

Another example of this is a modern car that has more than 100 sensors to monitor multiple functions such as fuel level, radar sensors and ultra sensors for close range detections.

\subsection{Computing Power}
Thanks to Moore’s law, processor chips continue to shrink in size, while increasing computing power, which has risen three to four magnitude compared to mid-1990s. The advance in computational power that has inevitably made all this possible derives not only from Moore’s law but also from the discovery of graphics processing units (GPU)s.

GPUs were first designed to boost speed and processing power in gaming industries give gamers a rich, high-speed 3D visual experiences. These were found to be twenty to fifty times more efficient than traditional central processing units (CPUs) for machine learning and deep learning computations \cite{Yosinski2015}.

Open AI, a non-profit research organization that promotes AI safety, remarks- \begin{quote}
    \textit{Improvements in computing have been a key component of AI progress, so as long as this trend continues, it’s worth preparing for the implications of systems far outside today’s capabilities.}
\end{quote}

An example of this is the speech recognition task, where a computer has to perform millions of calculation per second for a system to learn and recognize patterns in the data — this task required tremendous computational power that, until quite recently wasn't available. In contrast, Marvin Minsky, a pioneering mathematician and leading scientist from MIT worked on AI in 1957 when computers were billions of time slower than they are now. Those machines were costly and provided merely a fraction of the performance and computing speed.

\subsection{Inventive Algorithms}
%TODO: Consider Changing to - Algorithmic Innovation%
The data explosion and advances in computing power has made it possible for better and refined algorithms, and enable more extensive datasets that algorithms can process at any given time for machine learning tasks. 

Traditionally, algorithms were programmed explicitly by humans to perform various tasks. Modern algorithms have become sophisticated to the point where they can facilitate machine learning and allow computers to learn on their own. Before there wasn’t adequate data, either structured or unstructured to train computers to perform tasks on their own, let alone develop sophisticated algorithms that allowed machines to train themselves. 

An example of this is the autonomous vehicles car that relies on an enriched visual dataset to construct its own map real-time and navigate the roads. Here, each frame of the video collected by a self-driving car must be enriched with data that identify objects such as a road sign, pedestrian, tree or sidewalks, in every frame.

According to AI expert and MIT Sloan professor Erik Brynjolfsson, there has been a radical improvement in the algorithm. Some of them were first introduced 30 or 40 years ago, they have now been tweaked and improved drastically by faster computers and more data. With this, It became more comfortable to know what works and doesn't work over time.

\subsection{Tasks}
\begin{itemize}
\item Classification
\item Regression
\item Similarity Matching
\item Clustering
\item Co-occurrence grouping
\item Profiling
\item Link prediction
\item Data Reduction
\item Causal Modelling
\end{itemize}

\subsection{Task Types}

Machine learning can be classified into three categories (i) Supervised or semi-supervised learning (ii) Unsupervised Learning and (iii) Reinforcement Learning.

\subsection*{Supervised learning}

Supervised learning \cite{sammut2011encyclopedia} refers to training a model using a labeled dataset, where some of the training examples have labels, but others don’t. It's a machine learning process that learns a function from an input type to an output type using data comprising examples that have both input and output values. Two typical examples of supervised learning are classification learning and regression. In these cases, the output types are respectively categorical (the classes) and numeric. 

\subsection*{Unsupervised Learning}

Unsupervised learning \cite{sammut2011encyclopedia} refers to any machine learning process that seeks to learn to find patterns in a dataset, typically an unlabeled dataset or learn structure in the absence of either an identified output. Three typical examples of unsupervised learning are clustering,  association rules, and self-organizing maps.

\subsection*{Reinforcement Learning}

Reinforcement learning \cite{sammut2011encyclopedia} refers to a large class of learning problems characteristic of autonomous agents interacting in an environment: sequential decision-making problems with delayed reward. Reinforcement-learning algorithms seek to learn a policy (mapping from states to actions) that maximizes the reward received over time.

Unlike in supervised learning problems, in reinforcement-learning problems, there are no labeled examples of correct and incorrect behavior. However, unlike unsupervised learning problems, a reward signal can be perceived. This technique is based on the way animals seem to learn in response to positive feedback and requires a massive amount of dataset \cite{sammut2011encyclopedia}.

\section{The Dawn of Neural Networks}
In mid-1990, Artificial neural networks were introduced by researchers, and they marked a paradigm shift of predictive modeling from the world of applied statistics towards computer science and machine learning \cite{efron2016computer}.

\section{Deep Learning Revolution}

Deep learning is a superset of the machine learning domain. It is fair to say that neural networks were reincarnated around 2010 with “deep learning” as a new name, largely a result of much faster and larger computing systems, in addition to some new ideas. \cite{efron2016computer}.

% TODO: NEED CUSTOM VISUALS & GRAPHICS %

In the last decade, deep learning has been applied to various technologies and applications that require large volumes of digital data for training and providing useful information (Chen and Lin, 2014). Recently, they have has been advancing the state-of-the-art in artificial intelligence, and had led to major breakthrough in many areas such as computer vision (CV) \cite{krizhevsky2012imagenet} \cite{karpathy2014large}, speech recognition \cite{graves2013speech} \cite{mohamed2012acoustic}, and natural language processing \cite{bengio2003neural} \cite{mikolov2013distributed} \cite{mikolov2010recurrent}.

Deep learning \cite{Polson2018} is a specific set of techniques from the broader field of machine learning that focus on the study and usage of deep artificial neural networks to learn structured representations of data. It is used for classifying patterns using large training data sets and multi-layer AI neural networks. It's primarily a method for machines to learn from data that are loosely modeled on the way a biological brain learns to solve problems, where each artificial neural unit is connected to many other such units, and the links can be statistically strengthened or decreased based on the data used to train the system. Each successive layer in a multi-layer network uses the output from the previous layer as input.

\section{Architecture Overview}

Now that we have seen various components of deep networks, this section gives a general overview of the four significant architectures of deep networks.

We broadly survey the types of neural network to provide general insight and take a look at the four major architecture of deep networks that are used in industry and academia: 

\begin{itemize}
\item Perceptron
\item  Multi-layered Perceptron
\item Convolutional Neural Networks (CNN)
\item Recurrent Neural Networks
\item AutoEncoders (Unsupervised Pretrained Networks)
\end{itemize}

\subsection{Perceptrons}

A neural network is a highly parametrized model \cite{efron2016computer}, inspired by the architecture of the human brain. It was widely promoted as a universal approximator—a machine that with enough data could learn any smooth predictive relationship.

\subsubsection*{Single Layer Perceptrons}

Perceptron Model is illustrated in Fig.~\ref{fig:perceptron}.
\begin{figure}[htbp]
\centering
\includegraphics[width=0.6\textwidth]{images/perceptron.png}
\caption{Neural network diagram with a single hidden layer. The hidden layer derives transformations of the inputs—nonlinear transformations of linear combinations—which are then used to model the output.}
\label{fig:perceptron}
\end{figure}

\subsubsection*{Multi Layer Perceptrons}

A feed-forward neural network is a collection of neurons arranged in a sequence of multiple layers, where each neuron receive as input from the previous layer and perform a simple calculation (e.g., compute a weighted sum of the input followed by a nonlinear activation function) \cite{efron2016computer}. The neurons of the network collectively perform a nonlinear mapping from the input data to the output data. This mapping feature is learned from the data by adjusting and adapting the weights of each neuron using a technique called backpropagation \cite{efron2016computer}.

Feed Forward Neural Network Fig.~\ref{fig:multi-layer}.
\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{images/NN.png}
\caption{Neural network diagram with three hidden layers and multiple outputs, suitable for the MNIST handwritten-digit problem. The input layer has p D 784 units. Such a network with hidden layer sizes .1024; 1024; 2048/, and particular choices of tuning parameters, achieves the state-of-the art error rate of 0:93.}
\label{fig:multi-layer}
\end{figure}

% TODO: ANN equations, weight calculation, activation units %

\subsection{Convolutional Neural Networks}
A convolutional neural network (CNN) is an image classification network in which the network preserves the hierarchical structure by learning internal feature representations and generalizing the features in tasks like object recognition and other computer vision problems. It is not restricted to images but also applicable for audio, speech and language processing problems \cite{Manaswi2018}.

An example of convolutional neural network Fig.~\ref{fig:CNN-1}.
\begin{figure}[htbp]
\centering
\includegraphics[width=0.7\textwidth]{images/CNN.png}
\caption{Convolutional Neural Network and computer vision}
\label{fig:CNN-1}
\end{figure}

CNN is a feed-forward deep neural network architecture comprised of a convolutional layer, each followed by a pooling layer, activation function,  batch normalization and fully connected layers. As an image passes through the network, it gets smaller and smaller due to max pooling operation. The final layer outputs the probabilities of class prediction.

Fig.~\ref{fig:CNN-1}.
\begin{figure}[htbp]
\centering
\includegraphics[width=0.7\textwidth]{images/cnn-architecture.png}
\caption{CNN Architecture}
\label{fig:CNN-1}
\end{figure}


The effectiveness of CNN in image recognition is one of the main reasons why the world recognizes the power of deep learning. As Figure 4-7 illustrates, CNN is good at building a position and rotation invariant features from raw image data. It has lead to significant advances in machine vision, which has critical applications for self-driving cars, robotics, drones, and treatments for the visually impaired.

\section{The Advent of Black Box Models}

Deep learning models are hard to interpret than most existing machine learning models \cite{Kahng2018} because of its large number of layers, myriads of parameters, multiple types of non-linear activation functions, optimization algorithms and randomized gradient descent training process; the deep neural network is often considered as “black box \cite{dlvwz}.”

Due to their internal complexities and non-linear structure \cite{Samek}, the underlying decision-making processes for why the models are achieving such performance are the challenge and sometimes mystifying to interpret. Therefore, in practice, users often use them as a black box and cannot explain how learning from input data was done nor how performance can be consistently ensured. It could also be detrimental when the model does not perform satisfactorily; users would not understand the causes or know how to fix the problem. Generally, the information stored in a neural network is a set of numerical weights and connections that provide no direct evidence as for how to the task is executed or what is the association between inputs and outputs.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.90\textwidth]{images/Black-box.png}
\caption{Black box example of neural network system}
\label{fig:blackbox}
\end{figure}

The opaque nature of these models also limits their usage and acceptance in high-end science and engineering applications since it is demanded to use methods and techniques based on functions that can be understood and validated. 

Another reason that complicates the use of the neural network is that there are no well-defined criteria for choosing a neural network structure and corresponding parameter selection \cite{dlvwz}. It mostly depends on the trial and error process. The appropriate selection of parameters can vary widely even when performing very similar tasks due to various reasons. These parameters, which include network structure, depth and width of hidden layers, error bound, learning rate, training algorithm, hidden layer size, and the data vector used are often selected in a trial-and-error process. Therefore, when designing neural networks, it is tough for beginners to select the right parameters and find correct general rules. While, even for deep learning experts, it largely remains a trial-and-error process. 

The problem stems from the fact that we cannot merely inspect the deep neural network to see how it works or how it performs parallel computations \cite{darksecretaimittr}. A network’s learning and reasoning are embedded in the behavior of thousands of simulated neurons, arranged in dozens or even hundreds of intricately interconnected layers. For example, in the CNN model, neurons in the first layer receive an input image (as RGB value), which performs computation and outputs a new signal. These signals are fed into the successive layer and so on until the final layer output is generated \cite{darksecretaimittr}. Additionally, there is a process known as backpropagation that adjusts the calculation of neurons to optimize the output.

\section{Societal Implications}

As society is built upon a fabric of expected behavior and mutual trust, it is imperative we design AI systems that respect and fit with the social norms and ensure that their decision making is consistent with the ethical judgment. 

With the growing influence of artificial intelligence in our lives and their impact on the public domain, there is a critical need to understand better their decisions and realize how these models operate.  In a societal context, the reasons for a decision matters a lot. For example, death caused intentionally (murder) vs. death caused unintentionally (manslaughter) are distinct crimes at the court of law. Similarly, a hiring decision made is based (directly or indirectly) on specific protected characteristics such as race, the socio-economic class has a bearing on its legality. However, currently, predictive models are not capable of reasoning this or provide any explanation \cite{molnar}.

In this section, we examine the social implications of the black box neural network model, including how the relative opacity is potentially endangering the social equality and its wide-ranging impact in various social domains. We focus our discussion on the societal implication through seven critical themes:

\begin{enumerate}
\item \textbf{Trust}
\item  \textbf{Transparency}
\item \textbf{Fairness and Inclusion}
\item \textbf{Causality}
\item \textbf{Safety}
\item \textbf{Privacy}
\item \textbf{Ethics \& Accountability}
\end{enumerate}

We selected these themes because they are overarching concerns in the public domain. We discuss the harmful effect in each of these areas and identify emerging challenges in the present and future. In our discussion about these themes, we also ask the underlying questions such as: what are the current social and economic challenges faced by the rapid integration of AI, and how we can build a deeper understanding of AI in the present time that will help us create a fair and equitable future? \cite{Solon2017}

The wide-ranging impact in these selected themes makes it essential to look at how these automated decision-making systems are being applied now in regulated industries, whom they are benefiting, whom they are undermining, and how they are structuring the socio-economic aspect of society and individuals \cite{ainow2016report}.

\subsection{Trust}

For humans, It is easier to trust a system that explains its decisions compared to a black box. Deep learning to be confidently rolled out by industries and governments, users want greater transparency through explanations and justification of its decisions \cite{molnar}. It is an essential condition, not only for risk management but also to establish greater trust from the general public as well as regulators and supervisors in financial services. 

\subsection{Transparency}

Transparency is the opposite of opacity or the notion of black box”. Transparency is considered here at the level of the entire model at the level of individual components such as parameters (decomposability), and the level of the training algorithm (algorithmic transparency). 

\subsection{Fairness and Inclusion}

It is important to ensure that predictions made by the models are unbiased and do not implicitly or explicitly replicate human bias or discriminate against protected groups \cite{ainow2016report}. An interpretable model can explain why it decided to deny a loan for a particular person, and it becomes transparent and more accessible for a human to judge whether the decision made by the model is based on a learned demographic or not.

\subsection{Causality}
The model should ensure that it considers only causal relationships.  The system can only be compromised if the inputs are proxies for a causal feature, but do not cause the outcome. Hence proxy features should be avoided as they make models vulnerable. (\cite{molnar})

\subsection{Ethics}

Ethical questions surrounding AI systems are wide-ranging, spanning creation, uses and outcomes. There are important questions about which set of values and interests are reflected in AI, as well as how machines can recognize values and ethical paradigms. AI ethics concerns broader social concerns about the effects of AI systems and the choices made by the people who developed it.

\subsection{Privacy}
Assuring that sensitive information in the data is protected and uncompromised is important for privacy and security \cite{molnar}. AI challenges current understandings of privacy and strains the laws and regulations we have in place to protect personal information. Established approaches to privacy have become less and less effective because they are focused on previous metaphors of computing, ones where adversaries were primarily human. AI systems’ intelligence, as such, depends on ingesting as much training data as possible. This primary objective is adverse to the goals of privacy. AI thus poses significant challenges to traditional efforts to minimize data collection and to reform government and industry surveillance practices. 

\subsection{Safety}
Interpretability is especially critical if we want to consider autonomous vehicle safety before deploying the system, e.g., if specific errors are unacceptable even during training like specific edge case testing for self-driving cars. This aspect extends beyond autonomous vehicles to general AI systems, weighing in what kind of understanding is most helpful for safety?

\subsection{Accountability}
Terminology is now further complicated by concerns over the accountability (Diakopoulos 2016) and fairness (O’Neil 2016) of modern AI systems which, while overlapping the issue of end-user trust, extend into ethical and legal domains. It is important to ensure that small changes in the input do not lead to large changes in the prediction.

\section{Explainable and Interpretable System}

Interpretability is defined as the point to which a human can understand the causes of a decision. Another definition is that Interpretability is the degree to which a human can consistently predict the model’s result \cite{molnar}. The higher the interpretability of a machine learning model, the easier it is for someone to comprehend why certain decisions or predictions have been made. A model is better interpretable than another model if its decisions are more accessible for a human to comprehend than decisions from the other model \cite{molnar}.
    
Interpretability is the process of generating humanly understandable explanations of why a deep learning model makes a particular decision. Since the learning system hides the entire decision process behind the complicated inner-workings of deep learning models, it becomes difficult to obtain interpretations and explanations for their decisions.

As deep learning become an indispensable tool for a wide range of applications, it is essential to be able to verify for a certain task that the accuracy results from the proper framing of a problem statement, and there is no exploitation of artifacts in the data. Techniques for interpreting and understanding what the model has learned have therefore become a key ingredient of a robust validation procedure \citep{taylor2006methods} \citep{hansen2011visual} \citep{bach2015pixel}. Interpretability is critical in applications such as medical diagnosis or self-driving cars, where the dependency of the model on the correct features should be guaranteed \citep{Caruana:2015:IMH:2783258.2788613} \citep{bojarski2017explaining}.

\subsection{Why Interpretability Matters}

While artificial intelligence (AI) has existed for over sixty years, but the real-world application has increased only in the last decade due to factors discussed in the above section: the recent surge in data and the rise in computing power together with better algorithms.

With the growing success of deep learning and neural networks, there is a corresponding need to be able to explain their decisions, which includes detecting model bias, establishing trust and building confidence about how it performs in a real-world situation.

Without a clear understanding of how and why a model works in a certain way, the development of these models relies on a time consuming trial-and-error process. Consequently, both researchers and practitioners are facing challenges with complex models that demand more transparency and explainable systems to better understanding and analysis of these models. Whether it's an financial decision, a medical decision or maybe a military decision, one cannot rely on a black box method. It's time to act on making these decisions more transparent and understandable before the technology becomes even more pervasive.

This issue of black box neural networks has been a primary concern and a significant focus of discourse in the last few years among researchers and practitioners \cite{Samek}. As deep learning spreads across domains, it is of paramount importance that we equip users of deep learning with tools for understanding when a model works correctly, when it fails, and ultimately how to improve its performance.

\iffalse
The 2017 report from the AI committee of the British parliament states that the development of the intelligence AI systems is a fundamental necessity if an AI system is to become an integral and trusted tool in the society. Whether it takes the shape of explainability, technological transparency or both depend on the context and stakes involved in its application. However, in most cases, the report ascertains, explainability to be a useful approach for the citizens and the consumers. Further, the report suggests that it is not acceptable to implement an artificial intelligence system that could have a substantial impact on an individual's life unless it can provide a full, comprehensive and satisfactory explanation for the decisions it makes. It means delaying their deployment for such use cases until a credible alternative solution is found.
\fi

\subsection{Explainable Artificial Intelligence}
XAI concept explains individual decisions, enables understanding of overall strengths and weaknesses, and conveys an understanding of how the system will behave in the future and how to correct the system’s mistakes.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.65\textwidth]{images/XAI Research-1-crop.png}
\caption{Emphasis and Scope of XAI Research}
\label{fig:xai-1}
\end{figure}

Explainable Artificial Intelligence (XAI) System:  There has been a wave of interest in explainable artificial intelligence (XAI) in recent years driven by DARPA’s initiative to fund XAI projects. Historically, there has been occasional interest in explanations of intelligent systems and automated decision making systems over the past decades with expert systems, designed to solve complex problems by reasoning through prior knowledge in the 1970s [165, 172], Bayesian networks and artificial neural networks [6] in the 1980s, and then the recommender systems in the 2000s [33, 81]. The recent successes and advancement of AI and machine learning for many large-scale applications and the use of increasingly complex and non-transparent algorithms, mainly deep learning, calls for another rise of interest for the need to better understand these systems.

\section{Deep Learning Visualization Research}

With the growing complexity of AI models, the critical need for understanding their inner-workings has increased. Visualization is potentially a powerful technique to fill such a critical need. It can help bring more insight into the often obfuscated complex AI systems.
% TODO: NEED VISUALS

More than ever, data visualization has become critical to the field of AI. It can potentially help three broad groups of practitioners and users of AI who stand to benefit from deep learning visualization and visual analytics: model developers, model users, and non-experts \ref{Choo2018}.

Deep learning visualization research is an interdisciplinary area involving both deep learning and visual analytics techniques \cite{Choo2018}. It is distributed across multiple related fields within and outside the deep learning and AI community. In academia, the primary venue for deep learning visualization research consists of two main groups (1) Information Visualization and Visual Analytics community; and (2) artificial intelligence and deep learning communities. 

In addition to that, since this area is relatively new and emerging \cite{Choo2018}, it has seen more attention at multiple conferences and workshops, such as Neural Information Processing Systems (NeurIPS), IEEE Transactions on Visualization, IEEE Information Visualization and Computer Graphics and ICML Workshop on Human Interpretability in ML as just to name a few examples.

\subsection{Information Visualization Elements}

Information visualization is the study and usage of visual representation of abstract data to reinforce human cognition. It can help with the discovery of unstructured actionable insight, exploring and understanding the patterns in the data to communicate essential aspects of your data set in a concise, easy-to-understand fashion.

Data analysis is an indispensable part of the machine learning project pipeline, in both academia and research; and industry projects. The AI development pipeline often begins with data exploration phase, also known as exploratory data analysis to help with data analysis and evaluate approaches for the problem at hand. It has primarily been done using fundamental data analysis approach in visualization such as histograms, plots, charts, graphs and other types.

\subsection{Visual Analytics System}
    
Visual Analytics combines information visualization and scientific visualization and focuses on the analytical reasoning enabled with interactive visual interfaces. It’s an amalgamation of computer science, information visualization, cognitive and perceptual sciences, interactive design and social science.

Visual Analytics system have been developed to inspect artificial neural networks since visual feedback is considered highly valuable by both the practitioners and researchers


\subsection{Visualization and Interpretability}

Visualization is a powerful tool to meet this critical need. It can be used to demystify Deep Learning and explain how AI techniques work. Interactive Visualization would help enhance the interpretability and explainability of the deep learning models. Interactive visualization or Visual Analytics, as a whole is emerging as a promising research field. It has the potential to provide an in-depth understanding of deep learning models work and help make their inner workings transparent.

