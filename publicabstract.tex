%
%  Time-stamp: "[publicabstract.tex] last modified by Scott Budge (scott) on 2011-08-09 (Tuesday, 9 August 2011) at 09:17:43 on goga"
%
%  Info: $Id$   USU
%  Revision: $Rev$
% $LastChangedDate$
% $LastChangedBy$
%

\begin{publicabstract}
% A space is needed before the text starts so that the first paragraph
% is indented properly.

Deep learning, a branch of machine learning has seen a radical improvement in many areas such as voice recognition, computer vision and natural language processing. Machine learning is now deeply embedded in business decisions, both routing and high stake because they improve the consistency and quality of decision making. These techniques have now been embraced in regulated domains such as employment, credit, and insurance, where decisions like who gains access to healthcare, who is approved for a loan and who gets hired for a job depends on the output of the machine learning model. However, do we understand how these systems work? Are they trustworthy? Can we detect bias in their models? With their growing complexity of deep learning models,  the critical need for understanding their inner-workings has increased. The lack of explanation regarding their decisions and the absence of control over their internal processes act as a critical impediment. This work discuss the societal implications of black box models and the need for interpretable and explainable systems to enable trust and transparency in the automated decision making system. We propose visualization techniques that justify the prediction of a visual classifier by providing visual evidence for the decision made by the model, namely: (i) Sensitivity analysis to  show which part of the  input image  is  most  relevant  to  the  classification decision, and (ii) Visualizing intermediate outputs to get a better insight on what the network has learned at successive layers. Finally, we discuss the necessity to address the issue of transparency, accountability and fairness in the AI system, and ensure that bias in the data doesn't get embedded in the systems we create. Thereupon this works is a contribution towards promoting and building a fair, safe and aligned AI system.


\end{publicabstract}


% Local Variables:
% TeX-master: "newhead"
% End:
