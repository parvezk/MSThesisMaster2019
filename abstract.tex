%
%  Time-stamp: "[abstract.tex] last modified by Scott Budge (scott) on 2017-01-10 (Tuesday, 10 January 2017) at 16:54:14 on goga.ece.usu.edu"
%
%  Info: $Id: abstract.tex 998 2017-03-21 16:44:33Z scott $   USU
%  Revision: $Rev: 998 $
% $LastChangedDate: 2017-03-21 10:44:33 -0600 (Tue, 21 Mar 2017) $
% $LastChangedBy: scott $
%

\begin{abstract}
% A space is needed before the text starts so that the first paragraph
% is indented properly.

Deep learning a branch of machine learning has seen a revolutionary improvement in many areas such as voice recognition, computer vision and natural language processing. It is being used to guide all sorts of crucial decisions in medicine, finance, manufacturing and beyond, including deciding who gains access to healthcare, who gets approved for a loan and who gets hired for a job. However, do we understand how these systems work? Are they trustworthy? Can we detect bias in their models? The lack of explanation regarding the model decisions and absence of control over their internal processes act as a critical impediment. I discuss the societal implications of black box models and the need for interpretable and explainable systems. This work propose visualization techniques that justify the prediction of a visual classificators by providing visual evidence for their decision, namely: (i) Sensitivity analysis to  show which part of the  input image  is  most  relevant  to  the  classification decision, and (ii) Activation graph visualizing intermediate outputs to get a better insight on what the network has learned. Finally, I discuss the exigency to address the issue of transparency, accountability and fairness in the AI system, and ensure that bias in the data doesn't get embedded in the systems we create.

\end{abstract}


% Local Variables:
% TeX-master: "newhead"
% End:
