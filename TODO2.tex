\textbf{Prototype [Vision Objective, Motivation, Function}]===========

Vision
A visual exploration tool targeted towards non-technical audience
A visual narrative of the inference process

- A visual exploration tool to explore the inference made by an image recognition model
- Provide rationale for decision made by the model
- Help users distill large, complex neural network models into compact, interactive graph visualizations

A visual exploration tool to interpret class predictions
Show builds up its understanding of images over many layers

Motivated by notions of interpretability and assessing trust in models, we evaluate DeepViz via human studies to show that they can be important tools for users to evaluate and place trust in automated systems.

Our approach is weakly supervised localization in context of CNN: where the task is to localize objects in images using only whole image/video frame class labels

Our goal is to build an interactive visualization tool for users to better
understand how neural networks build their hierarchical representation.

\textbf{INTERPRETABILITY}

Definitions of interpretability center around human understanding, but they vary in the aspect of the model to be understood: its internals [11], operations [4], mapping of data [28], or representation [37]. Although recent work has begun to operationalize interpretability [15], a formal, agreed-upon definition remains open [7, 22].


Loc- approach like CAM are highly class-discriminative, exclusively highlights the object regions, localizing target objects in scenes

As a result, important regions of the image which correspond
to any decision of interest are visualized in high-resolution
detail even if the image contains evidence for multiple possible concepts

\textbf{BACKGROUND}======================================
RELATED WORK

\textbf{METHODOLOGY}=========================

\textbf{PLEASE REFER RED HIGHLIGHTS IN PAPER}

\textbf{DESIGN GOALS}

VGG16: (CNN) that achieves top-5 accuracy of 89.5% on the ImageNet dataset that contains over 1.2 millions images across 1000 classes.

APPROACH: HEATMAP Concept
Get visual interpretation of which parts of the image more most
 *    responsible for a convnet's classification decision, using the
 *    gradient-based class activation map (CAM) method.
 
 \textbf{SYSTEM DESIGN}
 \textbf{Web-based / Experimental setup}
 Remove Tensorflow.js
 - Interactive interface to visualize classes attribution graphs of a model....
 - Deployment using cross-platform, lightweight web technologies...
 
 \textbf{DeepViz User Interface}
  The header of SUMMIT displays metadata about the image clas-
fication model being visualized, such as the model and dataset name, the number of classes, and the total number data instances within the dataset.RESULTS==========================================

For image classification, our visualizations help identify dataset bias () and lend insight into failures of current CNNs, showing that seemingly unreasonable predictions have reasonable explanations. For

We conduct user testing that show CAM explanation are class discriminative, and not only gelp  human establish trust, but also help untrained users successfully discern a stronger network from a weaker, even when both make identical predictions

TRUST: Thus our viz can help users place trust in a model that can localize better, just based on individual prediction explanations.

Viz helps accurately explain the function learned by the model.

DETECT BIAS: This experiment demonstrates that Grad-CAM can
help detect and remove biases in datasets, which is impor-tant not just for generalization, but also for fair and ethical
outcomes as more algorithmic decisions are made in society.

\textit{REFER HIGHLIGHTS FROM GRAD-CAM PAPER}

\textit{CONCLUSION PART OF THE FEATURE VIZ ARTICLE}

Feature Visualization Notes========================
There is a growing sense that neural networks need to be interpretable to humans. The field of neural network interpretability has formed in response to these concerns. As it matures, two major threads of research have begun to coalesce: feature visualization and attribution.

Feature visualization answers questions about what a network — or parts of a network — are looking for by generating examples.

Attribution 1 studies what part of an example is responsible for the network activating a particular way.

which help us understand what information a model keeps and what it throws away. 

We are only at the beginning of understanding which objectives are interesting, and there is a lot of room for more work in this area.

Visualizing features with optimization. Optimization also has the advantage of flexibility. This flexibility can also be helpful in visualizing how features evolve as the network trains. 

examine techniques to get diverse visualizations, understand how neurons interact, and avoid high frequency artifacts.

Study techniques for visualizing and understanding neural networks, it's important to be able to try your experiments on multiple models.

Fubon Notes===============================

Machine learning is now deeply embedded in business decisions both routine and high-stakes. Consumers' everyday interactions with businesses and their ability to gain access to critical opportunities depend on the output of machine learned models. These techniques have been embraced in regulated domains such as employment, credit, and insurance precisely because they promise to improve the consistency and quality of decision-making. Yet there is growing recognition that learning models from historical data can end up replicating the human biases they promised to stamp out. Over the past few years, algorithmic fairness has become a watchword for consumer advocates, regulators, policymakers, and businesses alike. Less well understood, however, are the many ways that machine learning figures into the far more quotidian business decisions that do not fall under any regulation, but nevertheless raise concerns with fairness, ranging from marketing and advertising to information retrieval and personalization. In this talk, Solon will offer a survey of the wide range of fairness concerns prompted by businesses' embrace of machine learning.

A fireside chat featuring Dr. Barocas in conversation with NYU Stern Professor Foster Provost, the Director of the Fubon Data Analytics and AI Initiative, will follow. We hope you can join us. 
