
Visual interpretability for deep learning: a survey∗ Quan-shi
Abstract: This paper reviews recent studies in understanding neural-network representations and learning neu- ral networks with interpretable/disentangled middle-layer representations. Although deep neural networks have exhibited superior performance in various tasks, interpretability is always Achilles’ heel of deep neural networks. At present, deep neural networks obtain high discrimination power at the cost of a low interpretability of their black-box representations. We believe that high model interpretability may help people break several bottlenecks of deep learning, e.g., learning from a few annotations, learning via human–computer communications at the semantic level, and semantically debugging network representations. We focus on convolutional neural networks (CNNs), and revisit the visualization of CNN representations, methods of diagnosing representations of pre-trained CNNs, approaches for disentangling pre-trained CNN representations, learning of CNNs with disentangled representations, and middle-to-end learning based on model interpretability. Finally, we discuss prospective trends in explainable artificial intelligence.

Guestrin’s group has also devised ways for image recognition systems to hint at their reasoning by highlighting the parts of an image that were most significant

-------

Fig. 3 presents widely shared examples of how
neural networks learn hierarchical features by showing neuron feature
visualizations. It is commonly thought that neurons in lower layers in
a network learn low-level features, such as edges and textures, while
neurons in later layers learn more complicated parts and objects, like
faces (Fig. 3). In our work, we crystallize this belief by leveraging
feature visualization to identify what features a model has detected, and
how they are related.

Interactive visualization for explaining models to non-experts using direct manipulation has also seen attention due to the pervasiveness of machine learning in modern society and general interest from the public [13, 19, 43].


Feature Visualization Notes========================
There is a growing sense that neural networks need to be interpretable to humans. The field of neural network interpretability has formed in response to these concerns. As it matures, two major threads of research have begun to coalesce: feature visualization and attribution.

Feature visualization answers questions about what a network — or parts of a network — are looking for by generating examples.

Attribution 1 studies what part of an example is responsible for the network activating a particular way.

which help us understand what information a model keeps and what it throws away. 

We are only at the beginning of understanding which objectives are interesting, and there is a lot of room for more work in this area.

Visualizing features with optimization. Optimization also has the advantage of flexibility. This flexibility can also be helpful in visualizing how features evolve as the network trains. 

examine techniques to get diverse visualizations, understand how neurons interact, and avoid high frequency artifacts.

Study techniques for visualizing and understanding neural networks, it's important to be able to try your experiments on multiple models.

Fubon Notes===============================

Machine learning is now deeply embedded in business decisions both routine and high-stakes. Consumers' everyday interactions with businesses and their ability to gain access to critical opportunities depend on the output of machine learned models. These techniques have been embraced in regulated domains such as employment, credit, and insurance precisely because they promise to improve the consistency and quality of decision-making. Yet there is growing recognition that learning models from historical data can end up replicating the human biases they promised to stamp out. Over the past few years, algorithmic fairness has become a watchword for consumer advocates, regulators, policymakers, and businesses alike. Less well understood, however, are the many ways that machine learning figures into the far more quotidian business decisions that do not fall under any regulation, but nevertheless raise concerns with fairness, ranging from marketing and advertising to information retrieval and personalization. In this talk, Solon will offer a survey of the wide range of fairness concerns prompted by businesses' embrace of machine learning.

A fireside chat featuring Dr. Barocas in conversation with NYU Stern Professor Foster Provost, the Director of the Fubon Data Analytics and AI Initiative, will follow. We hope you can join us.

History of deep learning================
https://www.technologyreview.com/s/540001/teaching-machines-to-understand-us/
https://beamandrew.github.io/deeplearning/2017/02/23/deep_learning_101_part1.html