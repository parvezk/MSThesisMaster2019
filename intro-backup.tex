%
%  This is an example of how a LaTeX thesis should be formatted.  This
%  document contains chapter 1 of the thesis.
%
\chapter{INTRODUCTION}
%%%%%%%% This line gets rid of page number on first page of text
\thispagestyle{empty}

%%%%%%%%%%%%%
%It is responsible for spearheading the latest advancement in artificial intelligence research.%

Deep learning has led to unprecedented breakthrough in many areas such as computer vision, voice recognition, and autonomous driving.  It has proved very powerful at solving large-scale real-world problems in recent years and is being in many large-scale information processing applications like image recognition, language translation and automated personalization. There is now hope that these same techniques will be able to diagnose deadly diseases,  make trading decisions, and do many other things that will potentially transform our lives and many industries. 

While a deep neural network learns efficient representations and enable superior performance, understanding these models remains a challenge due to their inherently opaque nature and unclear working mechanism. They are often considered as black box methods that perform assigned tasks for the users. Without a definite understanding of how and why a model works, it is difficult for a user to determine when a model works correctly, when it fails and how it can be further improved. Hence, users treat neural networks as black boxes and cannot explain how mapping from input to output data was done or determine reasons for its predictions. This lack of transparency acts as a drawback to their application involving high stake decision making, especially in regulated industries where it is required to use techniques that can be understood and validated.

Additionally, automated decisions made by these models have far-reaching societal implications: widening inequality among social class[] and race and underpinning bias and discrimination in their systems[]. Consequently, transparency and fairness problems have been gaining more attention[] lately and efforts are being made to make deep learning models more interpretable and controllable by humans[], including building models that can explain their decisions, establish trust and transparency in how they behave in the real-world and detect bias and for scientific curiosity [].

Deep learning models are harder to interpret than most machine learning models as they learn representations that are complicated to extract and present in a human-readable form.  While it is plausible for certain types of models but it is  not entirely true for a vision-based model like a convolutional neural network (CNN) because the representations learned by deep network like CNN are highly responsive to visualization, in large part because they are representations of visual concepts. This work proposes a visualization technique to understand the inference of an image recognition model on a given image. 

I build a visual exploration tool that explains the rationale for a classification decision to an end user. The tool jointly predicts a class label,  and shows why the predicted label is appropriate for a given image using visual evidence. Our visualization approach covers the following three components:

\begin{enumerate}
\item \textbf{Image Receptiveness} of class activation in an image. This step is useful to understand which region of an input image attributed most to the classification decision by localizing the detected feature or object in the image.

%\item \textbf{Visualizing filters} to show what visual pattern or concept each filter in a network is receptive to.

\item  \textbf{Activation Graph}: Visualizing intermediate outputs helps user understands how successive layers in neural network transform their input, and provide an insight into the individual network filters.

\end{enumerate}

The following section (chapter 2.1) presents an overview of the background and historical context of deep learning and cursory glance on the origin of neural networks.  Section 2.4 presents an overview of the common architectures of deep neural network.  Section 2.4 also discusses the problems and challenges of the black box models. Section 2.5 presents the central claim in a broader context of societal impact and their spanning implications. Section 2.7 highlights the importance of explainable and interpretable systems.  Chapter 3 presents the methodologies used in the research process, including research hypothesis, challenges, experimental setup and the technical design of the prototype.

%\footnote{}