


%%%%%%%% This line gets rid of the page number on the first page of text


\noindent
\begin{table}
\begin{adjustwidth}{-1cm}{}
\renewcommand\arraystretch{2.5}\arrayrulecolor{LightSteelBlue3}
\captionsetup{singlelinecheck=false, font=blue, labelfont=sc, labelsep=quad}
\caption{Timeline}\vskip 0ex
\begin{tabular}{@{\,}r <{\hskip 0pt} !{\foo} >{\raggedright\arraybackslash}p{14cm}}
\toprule
\addlinespace[0ex]

1943 & McCulloch and Pitts proposed the McCulloch-Pitts neuron model.    \\

1949 & Hebb postulated the first rule for self-organized learning. \\

1956 & The Dartmouth Artificial Intelligence Summer Research Project, a seminal AI event that motivated a generation of scientists to explore potentials of computing to match the human capabilities. \\

1962 & Frank Rosenblatt introduced the simple single layer neural network called Perceptron - the precursor of today’s learning algorithms for deep neural networks. \\

1962 & David Hubel and Torsten Wiesel published their work on Receptive Fields and Functional Architecture in the Cat’s Visual Cortex," which revealed the pattern of organization of brain cells that process vision.  \\

1969 &  Marvin Minsky and Seymour Papert published Perceptrons that demonstrated the limitations of a single layer perceptron and marked the beginning of an AI winter. \\

1979 &  Geoffrey Hinton and James Anderson organized the Parallel Models of Associative Memory workshop, attended by a new generation of neural network pioneers.   \\

1986 & Yann LeCun and Geoffrey Hinton perfect backpropagation to train neural networks that pass data through successive layers allowing them to learn more complex skills. \\

1987 & NeurIPS - The First Neural Information Processing Systems (NeurIPS) Conference was held at the Denver Tech Center. \\

1990 & At Bell Labs, LeCun uses backpropagation to train a network that recognize handwritten digits. AT\&T later employs it in machines that can read checks. \\

%2006 & Hinton’s research group at the University of Toronto developed ways to train much larger networks with tens of layers of artificial neurons. \\

2012 &  AlexNet, a neural network model by Alex Krizhevsky, Ilya Sutskever and Geoffrey Hinton won ILSVRC (ImageNet Large Scale Visual Recognition Challenge) image classification competition. It marked an essential breakthrough in deep learning. \\

2019 & Yoshua Bengio, Geoffrey Hinton and Yann LeCun, the fathers of deep learning, receive the 2018 ACM Turing Award for conceptual and engineering breakthroughs that have made deep neural networks a critical component of computing today.

\end{tabular}
\end{adjustwidth}
\end{table}
